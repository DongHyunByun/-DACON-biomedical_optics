{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\",index_col=0)\n",
    "test=pd.read_csv(\"test.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0값을 결측치로 바꾸기\n",
    "\n",
    "# train\n",
    "train_dst = train.filter(regex='_dst$', axis=1).replace(0, np.NaN)\n",
    "train_src = train.filter(regex='_src$', axis=1).replace(0, np.NaN)\n",
    "# test\n",
    "test_dst = test.filter(regex='_dst$', axis=1).replace(0, np.NaN)\n",
    "test_src = test.filter(regex='_src$', axis=1).replace(0, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#보간, 보간되지 않은 값(결측치)는 0으로\n",
    "\n",
    "# train\n",
    "train_dst = train_dst.interpolate(methods='linear', axis=1)\n",
    "train_src = train_src.interpolate(methods='linear', axis=1)\n",
    "train_dst.fillna(0, inplace=True) \n",
    "train_src.fillna(0, inplace=True) \n",
    "\n",
    "\n",
    "# test\n",
    "test_dst = test_dst.interpolate(methods='linear', axis=1)\n",
    "test_src = test_src.interpolate(methods='linear', axis=1)\n",
    "test_dst.fillna(0, inplace=True)\n",
    "test_src.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=pd.merge(train_src,train_dst,on=\"id\")\n",
    "train_X[\"rho\"]=train[\"rho\"]\n",
    "x_col=train_X.columns\n",
    "\n",
    "test_X=pd.merge(test_src,test_dst,on=\"id\")\n",
    "test_X[\"rho\"]=test[\"rho\"]\n",
    "X_col=test_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "train_x=train_X[[i for i in x_col]].as_matrix()\n",
    "y_col=train.columns[-4:]\n",
    "train_y=train[[i for i in y_col]].as_matrix()\n",
    "\n",
    "test_x=test_X[[i for i in x_col]].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(x,y):\n",
    "    model = tf.keras.Sequential()  #순차적 계층화 준비\n",
    "    model.add(layers.Dense(140, input_shape=(71,))) \n",
    "    model.add(layers.Activation('relu')) \n",
    "\n",
    "    model.add(layers.Dense(140))  \n",
    "    model.add(layers.Activation('relu'))\n",
    "\n",
    "    model.add(layers.Dense(140))  \n",
    "    model.add(layers.Activation('relu'))\n",
    "    \n",
    "    model.add(layers.Dense(4))\n",
    "\n",
    "    # 모델 구축하기\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=\"adam\",\n",
    "        metrics=['mae'])\n",
    "    \n",
    "    hist = model.fit(\n",
    "    x, y,\n",
    "    batch_size=100,  #100개에 한 번씩 업데이터 실행\n",
    "    epochs=800,     \n",
    "    validation_split=0.2,  \n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)],  \n",
    "      #'val_loss'를 monitor하여 감소하면 한 번 더(1) 참고 조기중지\n",
    "    verbose=1)\n",
    "    \n",
    "    return hist,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/800\n",
      "8000/8000 [==============================] - 3s 333us/sample - loss: 10.1768 - mae: 2.3800 - val_loss: 7.7067 - val_mae: 2.0835\n",
      "Epoch 2/800\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 6.8631 - mae: 1.9669 - val_loss: 6.2994 - val_mae: 1.8839\n",
      "Epoch 3/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.9143 - mae: 1.8274 - val_loss: 5.8568 - val_mae: 1.8169\n",
      "Epoch 4/800\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 5.7392 - mae: 1.8005 - val_loss: 5.8499 - val_mae: 1.8170\n",
      "Epoch 5/800\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 5.6938 - mae: 1.7951 - val_loss: 5.8004 - val_mae: 1.8101\n",
      "Epoch 6/800\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 5.7303 - mae: 1.8003 - val_loss: 6.0941 - val_mae: 1.8554\n",
      "Epoch 7/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.6549 - mae: 1.7883 - val_loss: 5.7744 - val_mae: 1.8055\n",
      "Epoch 8/800\n",
      "8000/8000 [==============================] - 0s 51us/sample - loss: 5.6490 - mae: 1.7881 - val_loss: 5.8261 - val_mae: 1.8112\n",
      "Epoch 9/800\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 5.6582 - mae: 1.7885 - val_loss: 5.7779 - val_mae: 1.8060\n",
      "Epoch 10/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.6215 - mae: 1.7811 - val_loss: 5.7830 - val_mae: 1.8072\n",
      "Epoch 11/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.6338 - mae: 1.7847 - val_loss: 5.7450 - val_mae: 1.8036\n",
      "Epoch 12/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.6060 - mae: 1.7784 - val_loss: 5.7633 - val_mae: 1.8044\n",
      "Epoch 13/800\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 5.6224 - mae: 1.7832 - val_loss: 5.7644 - val_mae: 1.8022\n",
      "Epoch 14/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.6192 - mae: 1.7834 - val_loss: 5.7202 - val_mae: 1.7975\n",
      "Epoch 15/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.6046 - mae: 1.7787 - val_loss: 5.7539 - val_mae: 1.8042\n",
      "Epoch 16/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.6131 - mae: 1.7816 - val_loss: 5.7730 - val_mae: 1.8037\n",
      "Epoch 17/800\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 5.6018 - mae: 1.7798 - val_loss: 5.7763 - val_mae: 1.8071\n",
      "Epoch 18/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.6015 - mae: 1.7800 - val_loss: 5.7378 - val_mae: 1.8003\n",
      "Epoch 19/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.6513 - mae: 1.7878 - val_loss: 5.8753 - val_mae: 1.8258\n",
      "Epoch 20/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5949 - mae: 1.7781 - val_loss: 5.7104 - val_mae: 1.7941\n",
      "Epoch 21/800\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 5.5795 - mae: 1.7759 - val_loss: 5.7725 - val_mae: 1.8043\n",
      "Epoch 22/800\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 5.5790 - mae: 1.7776 - val_loss: 5.8268 - val_mae: 1.8161\n",
      "Epoch 23/800\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 5.5779 - mae: 1.7747 - val_loss: 5.7299 - val_mae: 1.7993\n",
      "Epoch 24/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5858 - mae: 1.7765 - val_loss: 5.7662 - val_mae: 1.8042\n",
      "Epoch 25/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5882 - mae: 1.7760 - val_loss: 5.8259 - val_mae: 1.8135\n",
      "Epoch 26/800\n",
      "8000/8000 [==============================] - 0s 36us/sample - loss: 5.6106 - mae: 1.7813 - val_loss: 5.8203 - val_mae: 1.8126\n",
      "Epoch 27/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5825 - mae: 1.7773 - val_loss: 5.7956 - val_mae: 1.8117\n",
      "Epoch 28/800\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 5.5870 - mae: 1.7768 - val_loss: 5.7738 - val_mae: 1.8066\n",
      "Epoch 29/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5837 - mae: 1.7774 - val_loss: 5.7504 - val_mae: 1.8037\n",
      "Epoch 30/800\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 5.5684 - mae: 1.7756 - val_loss: 5.7543 - val_mae: 1.8011\n",
      "Epoch 31/800\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 5.5723 - mae: 1.7746 - val_loss: 5.7254 - val_mae: 1.7981\n",
      "Epoch 32/800\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 5.5742 - mae: 1.7743 - val_loss: 5.7497 - val_mae: 1.8048\n",
      "Epoch 33/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5453 - mae: 1.7708 - val_loss: 5.7517 - val_mae: 1.7994\n",
      "Epoch 34/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5979 - mae: 1.7803 - val_loss: 5.7386 - val_mae: 1.7999\n",
      "Epoch 35/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5975 - mae: 1.7791 - val_loss: 5.7396 - val_mae: 1.8010\n",
      "Epoch 36/800\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 5.5895 - mae: 1.7778 - val_loss: 5.7584 - val_mae: 1.8043\n",
      "Epoch 37/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.5497 - mae: 1.7710 - val_loss: 5.7572 - val_mae: 1.8025\n",
      "Epoch 38/800\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 5.6010 - mae: 1.7781 - val_loss: 5.7560 - val_mae: 1.8057\n",
      "Epoch 39/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.5526 - mae: 1.7715 - val_loss: 5.7502 - val_mae: 1.8013\n",
      "Epoch 40/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5595 - mae: 1.7716 - val_loss: 5.7301 - val_mae: 1.7980\n",
      "Epoch 41/800\n",
      "8000/8000 [==============================] - 0s 30us/sample - loss: 5.5523 - mae: 1.7729 - val_loss: 5.8028 - val_mae: 1.8070\n",
      "Epoch 42/800\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 5.5501 - mae: 1.7702 - val_loss: 5.7145 - val_mae: 1.7966\n",
      "Epoch 43/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.5645 - mae: 1.7735 - val_loss: 5.7184 - val_mae: 1.7953\n",
      "Epoch 44/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5686 - mae: 1.7763 - val_loss: 5.7482 - val_mae: 1.7985\n",
      "Epoch 45/800\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 5.5634 - mae: 1.7723 - val_loss: 5.7273 - val_mae: 1.7958\n",
      "Epoch 46/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.5697 - mae: 1.7743 - val_loss: 5.8109 - val_mae: 1.8126\n",
      "Epoch 47/800\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 5.5609 - mae: 1.7730 - val_loss: 5.7606 - val_mae: 1.8050\n",
      "Epoch 48/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.5628 - mae: 1.7745 - val_loss: 5.7413 - val_mae: 1.7996\n",
      "Epoch 49/800\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 5.5584 - mae: 1.7730 - val_loss: 5.7337 - val_mae: 1.7982\n",
      "Epoch 50/800\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 5.5506 - mae: 1.7713 - val_loss: 5.7698 - val_mae: 1.8053\n",
      "Epoch 51/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5636 - mae: 1.7725 - val_loss: 5.7323 - val_mae: 1.7988\n",
      "Epoch 52/800\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 5.5635 - mae: 1.7742 - val_loss: 5.7888 - val_mae: 1.8089\n",
      "Epoch 53/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.5537 - mae: 1.7722 - val_loss: 5.7304 - val_mae: 1.7956\n",
      "Epoch 54/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.5601 - mae: 1.7732 - val_loss: 5.7251 - val_mae: 1.7971\n",
      "Epoch 55/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.5659 - mae: 1.7754 - val_loss: 5.7586 - val_mae: 1.8020\n",
      "Epoch 56/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5908 - mae: 1.7791 - val_loss: 5.7327 - val_mae: 1.7991\n",
      "Epoch 57/800\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 5.5335 - mae: 1.7679 - val_loss: 5.7427 - val_mae: 1.8005\n",
      "Epoch 58/800\n",
      "8000/8000 [==============================] - 0s 62us/sample - loss: 5.5540 - mae: 1.7721 - val_loss: 5.7479 - val_mae: 1.8006\n",
      "Epoch 59/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/sample - loss: 5.5335 - mae: 1.7691 - val_loss: 5.7445 - val_mae: 1.7997\n",
      "Epoch 60/800\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 5.5617 - mae: 1.7734 - val_loss: 5.7389 - val_mae: 1.7955\n",
      "Epoch 61/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5608 - mae: 1.7739 - val_loss: 5.7261 - val_mae: 1.7966\n",
      "Epoch 62/800\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 5.5564 - mae: 1.7740 - val_loss: 5.7577 - val_mae: 1.8043\n",
      "Epoch 63/800\n",
      "8000/8000 [==============================] - 0s 30us/sample - loss: 5.5341 - mae: 1.7705 - val_loss: 5.7586 - val_mae: 1.8033\n",
      "Epoch 64/800\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 5.5423 - mae: 1.7708 - val_loss: 5.7164 - val_mae: 1.7951\n",
      "Epoch 65/800\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 5.5258 - mae: 1.7669 - val_loss: 5.7383 - val_mae: 1.7980\n",
      "Epoch 66/800\n",
      "8000/8000 [==============================] - 0s 36us/sample - loss: 5.5527 - mae: 1.7726 - val_loss: 5.8319 - val_mae: 1.8146\n",
      "Epoch 67/800\n",
      "8000/8000 [==============================] - 0s 42us/sample - loss: 5.5628 - mae: 1.7725 - val_loss: 5.7647 - val_mae: 1.8038\n",
      "Epoch 68/800\n",
      "8000/8000 [==============================] - 0s 57us/sample - loss: 5.5406 - mae: 1.7706 - val_loss: 5.7248 - val_mae: 1.7985\n",
      "Epoch 69/800\n",
      "8000/8000 [==============================] - 1s 64us/sample - loss: 5.5341 - mae: 1.7692 - val_loss: 5.7466 - val_mae: 1.8034\n",
      "Epoch 70/800\n",
      "8000/8000 [==============================] - 0s 47us/sample - loss: 5.5279 - mae: 1.7690 - val_loss: 5.7265 - val_mae: 1.7970\n"
     ]
    }
   ],
   "source": [
    "hist,model=dnn(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.146208  3.9374049 8.94915   2.9461067]\n",
      " [8.024466  3.9235132 8.877751  2.8748875]\n",
      " [7.753181  3.9041235 8.95191   2.8278854]\n",
      " ...\n",
      " [7.842301  3.8179266 8.924166  2.7528546]\n",
      " [7.5813203 3.8894687 8.400471  2.79084  ]\n",
      " [7.994288  4.0006204 9.293969  2.8989902]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=[\"hhb\",\"hbo2\",\"ca\",\"na\"])\n",
    "\n",
    "for i in range(10000):\n",
    "    df=df.append({\"hhb\":y_pred[i][0],\"hbo2\":y_pred[i][1],\n",
    "                  \"ca\":y_pred[i][2],\"na\":y_pred[i][3]},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"myfile3.csv\",mode=\"w\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
